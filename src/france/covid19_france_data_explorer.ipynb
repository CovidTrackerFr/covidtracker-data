{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nLICENSE MIT\\n2021\\nGuillaume Rozier\\nWebsite : http://www.covidtracker.fr\\nMail : guillaume.rozier@telecomnancy.net\\n\\nREADME:\\nThis file contains scripts that download data from data.gouv.fr and then process it to build many graphes.\\nI'm currently cleaning the code, please ask me if something is not clear enough.\\n\\nThe charts are exported to 'charts/images/france'.\\nData is download to/imported from 'data/france'.\\nRequirements: please see the imports below (use pip3 to install them).\\n\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "LICENSE MIT\n",
    "2021\n",
    "Guillaume Rozier\n",
    "Website : http://www.covidtracker.fr\n",
    "Mail : guillaume.rozier@telecomnancy.net\n",
    "\n",
    "README:\n",
    "This file contains scripts that download data from data.gouv.fr and then process it to build many graphes.\n",
    "I'm currently cleaning the code, please ask me if something is not clear enough.\n",
    "\n",
    "The charts are exported to 'charts/images/france'.\n",
    "Data is download to/imported from 'data/france'.\n",
    "Requirements: please see the imports below (use pip3 to install them).\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import france_data_management as data\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "show_charts = False\n",
    "PATH_STATS = \"../../data/france/stats/\"\n",
    "PATH = \"../../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36it [00:15,  1.95it/s]                      "
     ]
    }
   ],
   "source": [
    "data.download_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regions_meta = pd.read_csv(PATH+\"data/france/population_grandes_regions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.download_data_obepine()\n",
    "#df_obepine = data.import_data_obepine()\n",
    "#df_obepine_france = df_obepine.groupby(\"Date\").mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adm_hosp_clage = data.import_data_hosp_ad_age()\n",
    "#df_adm_hosp_clage[\"jour\"] = pd.to_datetime(df_adm_hosp_clage.Semaine+\"-6\", format=\"%Y-S%U-%w\").fillna(0)\n",
    "df_adm_hosp_clage_france = df_adm_hosp_clage.groupby([\"jour\", \"cl_age90\"]).sum().fillna(0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A/Users/guillaumerozier/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "\n",
      " 38%|███▊      | 3/8 [01:12<02:00, 24.09s/it]\u001b[A\n",
      " 75%|███████▌  | 6/8 [01:16<00:34, 17.27s/it]\u001b[A\n",
      "15it [01:16, 12.09s/it]                      \u001b[A\n",
      "21it [01:29,  9.12s/it]\u001b[A\n",
      "28it [08:37, 24.72s/it]\u001b[A\n",
      "36it [08:37, 17.32s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "df, df_confirmed, dates, df_new, df_tests, df_deconf, df_sursaud, df_incid, df_tests_viros = data.import_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_france = data.import_data_new().groupby(\"jour\").sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vue_ensemble = data.import_data_vue_ensemble()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_education = data.import_data_education()\n",
    "\n",
    "df_niveaux_scolaires_fra = data.download_and_import_data_niveaux_scolaires_fra()\n",
    "df_niveaux_scolaires_reg = data.download_and_import_data_niveaux_scolaires_reg()\n",
    "df_niveaux_scolaires_dep = data.download_and_import_data_niveaux_scolaires_dep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_vacsi_a = data.import_data_vacsi_a_fra()\n",
    "#df_vacsi_a_reg = data.import_data_vacsi_a_reg()\n",
    "#df_vacsi_a_dep = data.import_data_vacsi_a_dep()\n",
    "\n",
    "df_vacsi = data.import_data_vacsi_fra() #df_vacsi_a.groupby(\"jour\").sum().reset_index()\n",
    "df_vacsi_reg = data.import_data_vacsi_reg() #df_vacsi_a_reg.groupby([\"jour\", \"reg\"]).sum().reset_index()\n",
    "df_vacsi_reg = df_vacsi_reg.merge(df_regions_meta, left_on=\"reg\", right_on=\"code\").rename({\"n_tot_dose1\": \"n_cum_dose1\"}, axis=1)\n",
    "\n",
    "df_vacsi_dep = data.import_data_vacsi_dep().rename({\"n_tot_dose1\": \"n_cum_dose1\"}, axis=1)\n",
    "#df_vacsi_a_dep.groupby([\"jour\", \"dep\"]).sum().reset_index().rename({\"n_tot_dose1\": \"n_cum_dose1\"}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'epci'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-29f88c326fb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_metro\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_data_metropoles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_metro\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"jour\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_metro\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sg\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_metro_65\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_metro\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_metro\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cl_age65\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m65\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_metro_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_metro\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_metro\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cl_age65\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Education/Covid-19_new/covidtracker-data/src/france/france_data_management.py\u001b[0m in \u001b[0;36mimport_data_metropoles\u001b[0;34m()\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0mepci\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'data/france/metropole-epci.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\";\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"'windows-1252'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0mdf_metro\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_metro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepci\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'epci'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'EPCI'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'EPCI'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf_metro\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m   7347\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7348\u001b[0m             \u001b[0mindicator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7349\u001b[0;31m             \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7350\u001b[0m         )\n\u001b[1;32m   7351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mindicator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     )\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         ) = self._get_merge_keys()\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         \u001b[0;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_get_merge_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    986\u001b[0m                         \u001b[0mright_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlk\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 988\u001b[0;31m                         \u001b[0mleft_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    989\u001b[0m                         \u001b[0mjoin_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1772\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1773\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1774\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1776\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'epci'"
     ]
    }
   ],
   "source": [
    "df_metro = data.import_data_metropoles()\n",
    "df_metro[\"jour\"] = df_metro[\"sg\"].map(lambda x: x[11:])\n",
    "\n",
    "df_metro_65 = df_metro[df_metro[\"cl_age65\"] == 65]\n",
    "df_metro_0 = df_metro[df_metro[\"cl_age65\"] == 0]\n",
    "metropoles = list(dict.fromkeys(list(df_metro['Metropole'].dropna().values))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tests_viros_enrichi = data.import_data_tests_viros()\n",
    "df_tests_viros_enrichi = df_tests_viros_enrichi.drop(\"regionName_y\", axis=1).rename({\"regionName_x\": \"regionName\"}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_incid_clage = df_incid.copy()\n",
    "\n",
    "df_incid_fra_clage = data.import_data_tests_sexe()\n",
    "df_incid_fra = df_incid_fra_clage[df_incid_fra_clage[\"cl_age90\"]==0]\n",
    "df_france = df.groupby([\"jour\"]).sum().reset_index()\n",
    "df_incid = df_incid[df_incid.cl_age90 == 0]\n",
    "\n",
    "df_sursaud_france = df_sursaud.groupby([\"date_de_passage\"]).sum().reset_index()\n",
    "df_sursaud_regions = df_sursaud.groupby([\"date_de_passage\", \"regionName\"]).sum().reset_index()\n",
    "\n",
    "#df_new_france = df_new.groupby([\"jour\"]).sum().reset_index()\n",
    "df_new_regions = df_new.groupby([\"jour\", \"regionName\"]).sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_incid_clage_regions = df_incid_clage.groupby([\"regionName\", \"jour\", \"cl_age90\"]).sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tests_viros_regions = df_tests_viros_enrichi.groupby([\"regionName\", \"jour\", \"cl_age90\"]).sum().reset_index()\n",
    "df_tests_viros_france = df_tests_viros_enrichi.groupby([\"jour\", \"cl_age90\"]).sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hosp_clage = data.import_data_hosp_clage()\n",
    "df_hosp_clage_france = df_hosp_clage.groupby([\"jour\", \"cl_age90\"]).sum().reset_index()\n",
    "df_hosp_clage_regions = df_hosp_clage.groupby([\"regionName\", \"jour\", \"cl_age90\"]).sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "departements = list(dict.fromkeys(list(df_incid['dep'].values)))\n",
    "regions = list(dict.fromkeys(list(df_incid['regionName'].dropna().values))) \n",
    "clage_list = list(dict.fromkeys(list(df_incid_fra_clage['cl_age90'].dropna().values))) \n",
    "\n",
    "df_regions = df.groupby([\"jour\", \"regionName\"]).sum().reset_index()\n",
    "df_incid_regions = df_incid.groupby([\"jour\", \"regionName\"]).sum().reset_index()\n",
    "\n",
    "\n",
    "zone_a = [\"zone_a\", \"01\", \"03\", \"07\", \"15\", \"16\", \"17\", \"19\", \"21\", \"23\", \"24\", \"25\", \"26\", \"33\", \"38\", \"39\", \"40\", \"42\", \"43\", \"47\", \"58\", \"63\", \"64\", \"69\", \"70\", \"71\", \"73\", \"74\", \"79\", \"86\", \"90\"]\n",
    "zone_b = [\"zone_b\", \"02\", \"04\", \"05\", \"06\", \"08\", \"10\", \"13\", \"14\", \"18\", \"22\", \"27\", \"28\", \"29\", \"35\", \"36\", \"37\", \"41\", \"44\", \"45\", \"49\", \"50\", \"51\", \"52\", \"53\", \"54\", \"55\", \"56\", \"57\", \"59\", \"60\", \"61\", \"62\", \"67\", \"68\", \"72\", \"76\", \"80\", \"83\", \"84\", \"85\", \"88\"]\n",
    "zone_c = [\"zone_c\", \"09\", \"11\", \"12\", \"30\", \"31\", \"32\", \"34\", \"46\", \"48\", \"65\", \"66\", \"75\", \"77\", \"78\", \"81\", \"82\", \"91\", \"92\", \"93\", \"94\", \"95\"]\n",
    "\n",
    "confines_mars_2021 = [\"confines_mars_2021\", \"02\", \"06\", \"27\", \"59\", \"60\", \"62\", \"75\", \"76\", \"77\", \"78\", \"80\", \"91\", \"92\", \"93\", \"94\", \"95\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_opendata_indicateurs = data.download_and_import_opendata_indicateurs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(data_incid=pd.DataFrame(), data_hosp=pd.DataFrame(), data_sursaud=pd.DataFrame(), data_new=pd.DataFrame(), data_vue_ensemble=pd.DataFrame(), data_metropole=pd.DataFrame(), data_vacsi=pd.DataFrame(), data_obepine=pd.DataFrame(), data_opendata_indicateurs=pd.DataFrame(), mode=\"\", export_jour=False, taux_croissance=False):## Incidence\n",
    "        \n",
    "    dict_data = {}\n",
    "    \n",
    "    if export_jour:\n",
    "        dict_data[\"jour_incid\"] = list(data_incid.jour)\n",
    "        dict_data[\"jour_hosp\"] = list(data_hosp.jour)\n",
    "        dict_data[\"jour_new\"] = list(data_new.jour)\n",
    "        dict_data[\"jour_sursaud\"] = list(data_sursaud.date_de_passage)\n",
    "        dict_data[\"jour_metropoles\"] = list(data_metropole.jour.unique())\n",
    "        dict_data[\"jour_vacsi\"] = list(data_vacsi.jour)\n",
    "        dict_data[\"jour_obepine\"] = \"\" #list(data_obepine.Date)\n",
    "        \n",
    "    if (taux_croissance) and (len(data_incid)>0):\n",
    "        cas = data_incid[\"P\"].fillna(0)\n",
    "        taux_croissance_cas = ((cas-cas.shift(7))/cas.shift(7).replace(0, None)).fillna(0) * 100\n",
    "        taux_croissance_cas[taux_croissance_cas>200]=200\n",
    "        taux_croissance_cas[taux_croissance_cas<-200]=-200\n",
    "        \n",
    "        cas_rolling = data_incid[\"P\"].rolling(window=7, center=True).mean().fillna(0)\n",
    "        taux_croissance_cas_rolling = ((cas_rolling-cas_rolling.shift(7))/cas_rolling.shift(7).replace(0, None)).fillna(0) * 100\n",
    "        dict_data[\"croissance_cas\"] = {\"jour_nom\": \"jour_incid\", \"valeur\": list(round(taux_croissance_cas, 1))}\n",
    "        dict_data[\"croissance_cas_rolling7\"] = {\"jour_nom\": \"jour_incid\", \"valeur\": list(round(taux_croissance_cas_rolling, 1))}\n",
    "        \n",
    "        tests = data_incid[\"T\"].rolling(window=7).mean().fillna(0)\n",
    "        taux_croissance_tests= ((tests-tests.shift(7))/tests.shift(7).replace(0, None)).fillna(0) * 100\n",
    "        dict_data[\"croissance_tests\"] = {\"jour_nom\": \"jour_incid\", \"valeur\": list(round(taux_croissance_tests, 1))}\n",
    "        dict_data[\"croissance_tests_rolling7\"] = {\"jour_nom\": \"jour_incid\", \"valeur\": list(round(taux_croissance_tests.rolling(window=7, center=True).mean().fillna(0), 1))}\n",
    "        \n",
    "    if (taux_croissance) and (len(data_hosp)>0):\n",
    "        hospitalisations = data_hosp.hosp.fillna(0)\n",
    "        taux_croissance_hospitalisations = ((hospitalisations-hospitalisations.shift(7))/hospitalisations.shift(7).replace(0, None)).fillna(0) * 100\n",
    "        dict_data[\"croissance_hospitalisations\"] = {\"jour_nom\": \"jour_hosp\", \"valeur\": list(round(taux_croissance_hospitalisations, 1))}\n",
    "        dict_data[\"croissance_hospitalisations_rolling7\"] = {\"jour_nom\": \"jour_hosp\", \"valeur\": list(round(taux_croissance_hospitalisations.rolling(window=7, center=True).mean().fillna(0), 1))}\n",
    "        \n",
    "        reanimations = data_hosp.rea.fillna(0)\n",
    "        taux_croissance_reanimations = ((reanimations-reanimations.shift(7))/reanimations.shift(7).replace(0, None)).fillna(0) * 100\n",
    "        dict_data[\"croissance_reanimations\"] = {\"jour_nom\": \"jour_hosp\", \"valeur\": list(round(taux_croissance_reanimations, 1))}\n",
    "        dict_data[\"croissance_reanimations_rolling7\"] = {\"jour_nom\": \"jour_hosp\", \"valeur\": list(round(taux_croissance_reanimations.rolling(window=7, center=True).mean().fillna(0), 1))}\n",
    "    \n",
    "    if (taux_croissance) and (len(data_new)>0):\n",
    "        hospitalisations = data_new.incid_hosp.fillna(0)\n",
    "        taux_croissance_hospitalisations = ((hospitalisations-hospitalisations.shift(7))/hospitalisations.shift(7).replace(0, None)).fillna(0) * 100\n",
    "        \n",
    "        hospitalisations_rolling = data_new.incid_hosp.fillna(0).rolling(window=7, center=True).mean()\n",
    "        taux_croissance_hospitalisations_rolling = ((hospitalisations_rolling-hospitalisations_rolling.shift(7))/hospitalisations_rolling.shift(7).replace(0, None)).fillna(0) * 100\n",
    "        dict_data[\"croissance_incid_hospitalisations\"] = {\"jour_nom\": \"jour_new\", \"valeur\": list(round(taux_croissance_hospitalisations, 1))}\n",
    "        dict_data[\"croissance_incid_hospitalisations_rolling7\"] = {\"jour_nom\": \"jour_new\", \"valeur\": list(round(taux_croissance_hospitalisations_rolling, 1))}\n",
    "        \n",
    "        reanimations = data_new.incid_rea.fillna(0)\n",
    "        taux_croissance_reanimations = ((reanimations-reanimations.shift(7))/reanimations.shift(7).replace(0, None)).fillna(0) * 100\n",
    "        \n",
    "        reanimations_rolling = data_new.incid_rea.fillna(0).rolling(window=7, center=True).mean()\n",
    "        taux_croissance_reanimations_rolling = ((reanimations_rolling-reanimations_rolling.shift(7))/reanimations_rolling.shift(7).replace(0, None)).fillna(0) * 100\n",
    "        dict_data[\"croissance_incid_reanimations\"] = {\"jour_nom\": \"jour_new\", \"valeur\": list(round(taux_croissance_reanimations, 1))}\n",
    "        dict_data[\"croissance_incid_reanimations_rolling7\"] = {\"jour_nom\": \"jour_new\", \"valeur\": list(round(taux_croissance_reanimations_rolling, 1))}\n",
    "    \n",
    "    if (taux_croissance)and len(data_opendata_indicateurs):\n",
    "        dict_data[\"jour_spf_opendata\"] = list(data_opendata_indicateurs[[\"date\", \"conf_j1\"]].dropna().date)\n",
    "        \n",
    "        data_opendata_indicateurs[\"conf_j1_corrige\"] = data_opendata_indicateurs[\"conf\"].diff()\n",
    "        data_opendata_indicateurs.loc[data_opendata_indicateurs[\"date\"]==\"2021-05-20\", \"conf_j1_corrige\"] = 15000\n",
    "        \n",
    "        #cas_spf_opendata = data_opendata_indicateurs[\"conf_j1_corrige\"].fillna(0)\n",
    "        cas_spf_opendata = data_opendata_indicateurs[\"conf_j1\"].dropna()\n",
    "            \n",
    "        taux_croissance_cas_spf_opendata = ((cas_spf_opendata-cas_spf_opendata.shift(7))/cas_spf_opendata.shift(7).replace(0, None)).fillna(0) * 100\n",
    "        cas_spf_opendata_rolling = cas_spf_opendata.rolling(window=7, center=True).mean().fillna(0)\n",
    "        taux_croissance_cas_spf_opendata_rolling = ((cas_spf_opendata_rolling-cas_spf_opendata_rolling.shift(7))/cas_spf_opendata_rolling.shift(7).replace(0, None)).fillna(0) * 100\n",
    "        \n",
    "        dict_data[\"croissance_cas_spf_opendata\"] = {\"jour_nom\": \"jour_spf_opendata\", \"valeur\": list(round(taux_croissance_cas_spf_opendata, 1))}\n",
    "        dict_data[\"croissance_cas_spf_opendata_rolling7\"] = {\"jour_nom\": \"jour_spf_opendata\", \"valeur\": list(round(taux_croissance_cas_spf_opendata_rolling, 1))}\n",
    "        \n",
    "        cas_spf_opendata_corrige = data_opendata_indicateurs[[\"date\", \"conf_j1_corrige\"]]\n",
    "        cas_spf_opendata_corrige.loc[cas_spf_opendata_corrige.date == \"2021-05-02\", \"conf_j1_corrige\"] = cas_spf_opendata_corrige.loc[cas_spf_opendata_corrige.date == \"2021-04-25\", \"conf_j1_corrige\"].values[0] * 0.7\n",
    "        cas_spf_opendata_corrige.loc[cas_spf_opendata_corrige.date == \"2021-05-09\", \"conf_j1_corrige\"] = cas_spf_opendata_corrige.loc[cas_spf_opendata_corrige.date == \"2021-05-02\", \"conf_j1_corrige\"].values[0] * 0.7\n",
    "        cas_spf_opendata_corrige.loc[cas_spf_opendata_corrige.date == \"2021-05-14\", \"conf_j1_corrige\"] = 0.7 * cas_spf_opendata_corrige.loc[cas_spf_opendata_corrige.date == \"2021-05-07\"][\"conf_j1_corrige\"].values[0]\n",
    "        cas_spf_opendata_corrige.loc[cas_spf_opendata_corrige.date == \"2021-05-25\", \"conf_j1_corrige\"] = 0.7 * cas_spf_opendata_corrige.loc[cas_spf_opendata_corrige.date == \"2021-05-18\"][\"conf_j1_corrige\"].values[0]\n",
    "        cas_spf_opendata_corrige.loc[cas_spf_opendata_corrige.date == \"2021-07-15\", \"conf_j1_corrige\"] = 1.8 * cas_spf_opendata_corrige.loc[cas_spf_opendata_corrige.date == \"2021-07-08\"][\"conf_j1_corrige\"].values[0]\n",
    "        cas_spf_opendata_corrige.loc[cas_spf_opendata_corrige.date == \"2021-11-02\", \"conf_j1_corrige\"] = 1.1 * cas_spf_opendata_corrige.loc[cas_spf_opendata_corrige.date == \"2021-10-26\"][\"conf_j1_corrige\"].values[0]\n",
    "        cas_spf_opendata_corrige.loc[cas_spf_opendata_corrige.date == \"2021-11-12\", \"conf_j1_corrige\"] = 1.3 * cas_spf_opendata_corrige.loc[cas_spf_opendata_corrige.date == \"2021-11-05\"][\"conf_j1_corrige\"].values[0]\n",
    "        cas_spf_opendata_corrige.loc[cas_spf_opendata_corrige.date == \"2021-12-26\", \"conf_j1_corrige\"] = 1.5 * cas_spf_opendata_corrige.loc[cas_spf_opendata_corrige.date == \"2021-12-19\"][\"conf_j1_corrige\"].values[0]\n",
    "        cas_spf_opendata_corrige.loc[cas_spf_opendata_corrige.date == \"2021-01-01\", \"conf_j1_corrige\"] = 2 * cas_spf_opendata_corrige.loc[cas_spf_opendata_corrige.date == \"2021-12-25\"][\"conf_j1_corrige\"].values[0]\n",
    "        cas_spf_opendata_corrige = cas_spf_opendata_corrige[\"conf_j1_corrige\"].rolling(window=7).mean().fillna(0)\n",
    "        \n",
    "        dict_data[\"cas_spf_opendata_rolling_corrige\"] = {\"jour_nom\": \"jour_spf_opendata\", \"valeur\": list(round(cas_spf_opendata_corrige, 1))}\n",
    "\n",
    "    if(len(data_vacsi)>0):\n",
    "        n_cum_dose1 = data_vacsi[\"n_cum_dose1\"].fillna(0)\n",
    "        dict_data[\"n_cum_dose1\"] = {\"jour_nom\": \"jour_vacsi\", \"valeur\": list(n_cum_dose1)}\n",
    "        \n",
    "        n_dose1 = data_vacsi[\"n_dose1\"].rolling(window=7).mean().fillna(0)\n",
    "        dict_data[\"n_dose1\"] = {\"jour_nom\": \"jour_vacsi\", \"valeur\": list(round(n_dose1, 1))}\n",
    "    \n",
    "    if len(data_vue_ensemble)>0:\n",
    "        dict_data[\"jour_ehpad\"] = list(data_vue_ensemble.date)\n",
    "        deces_ehpad = data_vue_ensemble[\"total_deces_ehpad\"].diff().rolling(window=7).mean().fillna(0)\n",
    "        dict_data[\"deces_ehpad\"] = {\"jour_nom\": \"jour_ehpad\", \"valeur\": list(round(deces_ehpad,2))}\n",
    "        \n",
    "        cas_spf = data_vue_ensemble.total_cas_confirmes.diff().fillna(0)\n",
    "        cas_spf[cas_spf<0] = 0\n",
    "        cas_spf = cas_spf.replace(to_replace=0, method='ffill')\n",
    "        cas_spf_rolling = cas_spf.rolling(window=7).mean().fillna(0)\n",
    "        dict_data[\"cas_spf\"] = {\"jour_nom\": \"jour_ehpad\", \"valeur\": list(round(cas_spf_rolling, 2))}\n",
    "        dict_data[\"cas_spf_brut\"] = {\"jour_nom\": \"jour_ehpad\", \"valeur\": list(round(cas_spf, 2))}\n",
    "        \n",
    "    if len(data_opendata_indicateurs):\n",
    "        dict_data[\"jour_spf_opendata\"] = list(data_opendata_indicateurs[[\"date\", \"conf_j1\"]].dropna().date)\n",
    "        #cas_spf_opendata = data_opendata_indicateurs[\"conf_j1_corrige\"].fillna(0)\n",
    "        cas_spf_opendata = data_opendata_indicateurs[\"conf_j1\"].dropna()\n",
    "        cas_spf_opendata_rolling = cas_spf_opendata.rolling(window=7).mean().fillna(0)\n",
    "        \n",
    "        dict_data[\"cas_spf_opendata\"] = {\"jour_nom\": \"jour_spf_opendata\", \"valeur\": list(cas_spf_opendata)}\n",
    "        dict_data[\"cas_spf_opendata_rolling\"] = {\"jour_nom\": \"jour_spf_opendata\", \"valeur\": list(round(cas_spf_opendata_rolling, 2))}\n",
    "        \n",
    "        \n",
    "    if len(data_obepine)>0:\n",
    "        indicateur_obepine = data_obepine.Indicateur.fillna(0)\n",
    "        \n",
    "        dict_data[\"obepine\"] = {\"jour_nom\": \"jour_obepine\", \"jours\":list(data_obepine.Date), \"valeur\": list(round(indicateur_obepine, 2))}\n",
    "        \n",
    "    if len(data_incid)>0:\n",
    "        taux_incidence = data_incid[\"P\"].rolling(window=7).sum().fillna(0) * 100000 / float(data_incid[\"pop\"].values[0])\n",
    "        dict_data[\"incidence\"] = {\"jour_nom\": \"jour_incid\", \"valeur\": list(round(taux_incidence, 1))}\n",
    "\n",
    "        taux_positivite = (data_incid[\"P\"] / data_incid[\"T\"] * 100).rolling(window=7).mean().fillna(0)\n",
    "        dict_data[\"taux_positivite\"] = {\"jour_nom\": \"jour_incid\", \"valeur\": list(round(taux_positivite, 2))}\n",
    "        \n",
    "        taux_positivite = (data_incid[\"P\"].rolling(window=7).mean() / data_incid[\"T\"].rolling(window=7).mean() * 100).fillna(0)\n",
    "        dict_data[\"taux_positivite_rolling_before\"] = {\"jour_nom\": \"jour_incid\", \"valeur\": list(round(taux_positivite, 2))}\n",
    "    \n",
    "        cas = data_incid[\"P\"].rolling(window=7).mean().fillna(0)\n",
    "        dict_data[\"cas\"] = {\"jour_nom\": \"jour_incid\", \"valeur\": list(round(cas, 1))}\n",
    "        \n",
    "        cas = data_incid[\"P\"].fillna(0)\n",
    "        dict_data[\"cas_brut\"] = {\"jour_nom\": \"jour_incid\", \"valeur\": list(round(cas, 1))}\n",
    "        \n",
    "        cas_total = data_incid[\"P\"].sum()\n",
    "        dict_data[\"cas_total\"] = {\"jour_nom\": \"jour_incid\", \"valeur\": int(cas_total)}\n",
    "    \n",
    "        tests = data_incid[\"T\"].rolling(window=7).mean().fillna(0)\n",
    "        dict_data[\"tests\"] = {\"jour_nom\": \"jour_incid\", \"valeur\": list(round(tests, 1))}\n",
    "        \n",
    "        tests_total = data_incid[\"T\"].sum()\n",
    "        dict_data[\"tests_total\"] = {\"jour_nom\": \"jour_incid\", \"valeur\": int(tests_total)}\n",
    "        \n",
    "    if (len(data_metropole)>0) & (mode==\"metropoles\"):\n",
    "        taux_incidence = data_metropole[\"Ti\"].fillna(0)\n",
    "        dict_data[\"incidence\"] = {\"jour_nom\": \"jour_metropoles\", \"valeur\": list(round(taux_incidence, 1))}\n",
    "        \n",
    "    if (taux_croissance) and (len(data_hosp)>0):\n",
    "        hospitalisations_rolling = data_hosp.hosp.rolling(window=7).mean().fillna(0)\n",
    "        croissance_hospitalisations = ((hospitalisations_rolling-hospitalisations_rolling.shift(7))/hospitalisations_rolling.shift(7).replace(0, None)).fillna(0) * 100\n",
    "        dict_data[\"croissance_hospitalisations\"] = {\"jour_nom\": \"jour_hosp\", \"valeur\": list(round(croissance_hospitalisations, 1))}\n",
    "        \n",
    "        sc_rolling = data_hosp.rea.rolling(window=7).mean().fillna(0)\n",
    "        croissance_sc = ((sc_rolling-sc_rolling.shift(7))/sc_rolling.shift(7).replace(0, None)).fillna(0) * 100\n",
    "        dict_data[\"croissance_reanimations\"] = {\"jour_nom\": \"jour_hosp\", \"valeur\": list(round(croissance_sc, 1))}\n",
    "        \n",
    "    if len(data_hosp)>0:\n",
    "        hospitalisations = data_hosp[\"hosp\"].fillna(0)\n",
    "        dict_data[\"hospitalisations\"] = {\"jour_nom\": \"jour_hosp\", \"valeur\": list(hospitalisations)}\n",
    "\n",
    "        reanimations = data_hosp.rea.fillna(0)\n",
    "        dict_data[\"reanimations\"] = {\"jour_nom\": \"jour_hosp\", \"valeur\": list(reanimations)}\n",
    "        \n",
    "        saturation_rea = round(data_hosp[\"rea\"]/data_hosp[\"LITS\"].fillna(0)*100, 1)\n",
    "        dict_data[\"saturation_reanimations\"] = {\"jour_nom\": \"jour_hosp\", \"valeur\": list(saturation_rea)}\n",
    "    \n",
    "    if len(data_new)>0:\n",
    "        incid_hospitalisations = data_new.incid_hosp.rolling(window=7).mean().fillna(0)\n",
    "        dict_data[\"incid_hospitalisations\"] = {\"jour_nom\": \"jour_new\", \"valeur\": list(round(incid_hospitalisations, 1))}\n",
    "        \n",
    "        incid_hospitalisations_total = data_new.incid_hosp.sum()\n",
    "        dict_data[\"incid_hospitalisations_total\"] = {\"jour_nom\": \"jour_new\", \"valeur\": int(incid_hospitalisations_total)}\n",
    "\n",
    "        incid_reanimations = data_new.incid_rea.rolling(window=7).mean().fillna(0)\n",
    "        dict_data[\"incid_reanimations\"] = {\"jour_nom\": \"jour_new\", \"valeur\": list(round(incid_reanimations, 1))}\n",
    "        \n",
    "        incid_reanimations_total = data_new.incid_rea.sum()\n",
    "        dict_data[\"incid_reanimations_total\"] = {\"jour_nom\": \"jour_new\", \"valeur\": int(incid_reanimations_total)}\n",
    "    \n",
    "    if len(data_sursaud)>0:\n",
    "        nbre_acte_corona = data_sursaud.nbre_acte_corona.rolling(window=7).mean().fillna(0)\n",
    "        dict_data[\"nbre_acte_corona\"] = {\"jour_nom\": \"jour_sursaud\", \"valeur\": list(round(nbre_acte_corona, 1))}\n",
    "\n",
    "        nbre_pass_corona = data_sursaud.nbre_pass_corona.rolling(window=7).mean().fillna(0)\n",
    "        dict_data[\"nbre_pass_corona\"] = {\"jour_nom\": \"jour_sursaud\", \"valeur\": list(round(nbre_pass_corona,  1))}\n",
    "    \n",
    "    if len(data_new)>0:\n",
    "        deces_hospitaliers = data_new.incid_dc\n",
    "        taux_croissance_deces_hospitaliers = ((deces_hospitaliers-deces_hospitaliers.shift(7))/deces_hospitaliers.shift(7).replace(0, None)).fillna(0) * 100\n",
    "        \n",
    "        deces_hospitaliers_rolling = data_new.incid_dc.rolling(window=7, center=False).mean().fillna(0)\n",
    "        taux_croissance_deces_hospitaliers_rolling = ((deces_hospitaliers_rolling-deces_hospitaliers_rolling.shift(7))/deces_hospitaliers_rolling.shift(7).replace(0, None)).fillna(0) * 100\n",
    "        \n",
    "        dict_data[\"deces_hospitaliers\"] = {\"jour_nom\": \"jour_new\", \"valeur\": list(round(deces_hospitaliers_rolling, 1))}\n",
    "        dict_data[\"croissance_deces_hospitaliers\"] = {\"jour_nom\": \"jour_new\", \"valeur\": list(round(taux_croissance_deces_hospitaliers, 1))}\n",
    "        dict_data[\"croissance_deces_hospitaliers_rolling7\"] = {\"jour_nom\": \"jour_new\", \"valeur\": list(round(taux_croissance_deces_hospitaliers_rolling, 1))}\n",
    "        \n",
    "        deces_hospitaliers_total = data_new.incid_dc.sum()\n",
    "        dict_data[\"deces_hospitaliers_total\"] = {\"jour_nom\": \"jour_new\", \"valeur\": int(deces_hospitaliers_total)}\n",
    "    \n",
    "    if len(data_incid)>0:\n",
    "        population = data_incid[\"pop\"].values[0]\n",
    "        dict_data[\"population\"] = population\n",
    "\n",
    "    return dict_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_age(data_incid, data_hosp, data_adm_hosp_clage=pd.DataFrame(), export_jour=False):## Incidence\n",
    "    clage_tranches = [[\"9\", \"19\", \"29\", \"39\", \"49\", \"59\", \"69\", \"79\", \"89\", \"90\"], [\"9\", \"19\"], [\"29\", \"39\"], [\"49\", \"59\"], [\"69\", \"79\"], [\"89\", \"90\"]]\n",
    "    clage_noms = [\"tous\", \"19\", \"39\", \"59\", \"79\", \"90\"]\n",
    "    clage_noms_disp = [\"Tous âges\", \"0 à 19 ans\", \"20 à 39 ans\", \"40 à 59 ans\", \"60 à 79 ans\", \"Plus de 80 ans\"]\n",
    "    \n",
    "    dict_data = {}\n",
    "    \n",
    "    for (idx, clage) in enumerate(clage_tranches):\n",
    "        clage_nom = clage_noms[idx]\n",
    "        \n",
    "        data_incid_clage = data_incid[data_incid.cl_age90.isin(clage)].groupby(\"jour\").sum().reset_index()\n",
    "\n",
    "        dict_data[clage_nom] = {}\n",
    "\n",
    "        taux_incidence = data_incid_clage[\"P\"].rolling(window=7).sum().fillna(0) * 100000 / data_incid_clage[\"pop\"].values[0]\n",
    "        dict_data[clage_nom][\"incidence\"] = {\"jour_nom\": \"jour_incid\", \"valeur\": list(round(taux_incidence,0))}\n",
    "\n",
    "        taux_positivite = (data_incid_clage[\"P\"] / data_incid_clage[\"T\"] * 100).rolling(window=7).mean().fillna(0)\n",
    "        dict_data[clage_nom][\"taux_positivite\"] = {\"jour_nom\": \"jour_incid\", \"valeur\": list(round(taux_positivite,2))}\n",
    "\n",
    "        cas = data_incid_clage[\"P\"].rolling(window=7).mean().fillna(0)\n",
    "        dict_data[clage_nom][\"cas\"] = {\"jour_nom\": \"jour_incid\", \"valeur\": list(round(cas, 1))}\n",
    "        \n",
    "        taux_croissance_cas = ((cas-cas.shift(7))/cas.shift(7).replace(0, None)).fillna(0) * 100\n",
    "        taux_croissance_cas[taux_croissance_cas>200] = 200\n",
    "        taux_croissance_cas[taux_croissance_cas<-200] = -200\n",
    "        #taux_croissance_cas = taux_croissance_cas.rolling(window=7).mean().fillna(0)\n",
    "        dict_data[clage_nom][\"cas_croissance_hebdo\"] = {\"jour_nom\": \"jour_incid\", \"valeur\": list(round(taux_croissance_cas, 1))}\n",
    "\n",
    "        tests = data_incid_clage[\"T\"].rolling(window=7).mean().fillna(0)\n",
    "        dict_data[clage_nom][\"tests\"] = {\"jour_nom\": \"jour_incid\", \"valeur\": list(round(tests, 1))}\n",
    "        \n",
    "        population = data_incid_clage[\"pop\"].values[0]\n",
    "        dict_data[clage_nom][\"population\"] = population\n",
    "        \n",
    "        if (len(data_hosp)):  \n",
    "            data_hosp_clage = data_hosp[data_hosp.cl_age90.isin(clage)].groupby(\"jour\").sum().reset_index()\n",
    "            hospitalisations = data_hosp_clage.hosp.fillna(0)\n",
    "            dict_data[clage_nom][\"hospitalisations\"] = {\"jour_nom\": \"jour_hosp\", \"valeur\": list(hospitalisations)}\n",
    "\n",
    "            reanimations = data_hosp_clage.rea.fillna(0)\n",
    "            dict_data[clage_nom][\"reanimations\"] = {\"jour_nom\": \"jour_hosp\", \"valeur\": list(reanimations)}\n",
    "\n",
    "            deces_hospitaliers = data_hosp_clage.dc.diff().rolling(window=7).mean().fillna(0)\n",
    "            dict_data[clage_nom][\"deces_hospitaliers\"] = {\"jour_nom\": \"jour_hosp\", \"valeur\": list(round(deces_hospitaliers, 1))}\n",
    "        \n",
    "        if(len(data_adm_hosp_clage)):\n",
    "            df_adm_hosp_clage_temp = data_adm_hosp_clage[data_adm_hosp_clage.cl_age90.isin(clage)].groupby(\"jour\").sum().reset_index()\n",
    "            adm_hospitalisations = df_adm_hosp_clage_temp[\"NewAdmHospit\"]\n",
    "            dict_data[clage_nom][\"adm_hospitalisations\"] = {\"jour_nom\": \"jour_adm_hosp_clage\", \"valeur\": list(round(adm_hospitalisations/7, 1))}\n",
    "            \n",
    "    if export_jour:\n",
    "            dict_data[\"jour_incid\"] = list(data_incid.jour.unique())\n",
    "            dict_data[\"jour_hosp\"] = list(data_hosp.jour.unique())\n",
    "            dict_data[\"tranches\"] = clage_tranches\n",
    "            dict_data[\"tranches_noms\"] = clage_noms\n",
    "            dict_data[\"tranches_noms_affichage\"] = clage_noms_disp\n",
    "            \n",
    "            if(len(data_adm_hosp_clage)):\n",
    "                dict_data[\"jour_adm_hosp_clage\"] = list(data_adm_hosp_clage.jour.dt.strftime('%Y-%m-%d').unique())\n",
    "\n",
    "    return dict_data\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_niveaux_scolaires(data_incid_niveaux_scolaires, export_jour=False):\n",
    "    dict_data={}\n",
    "    clage_tranches = [\"0\", \"2\", \"5\", \"10\", \"14\", \"17\", \"18\"]\n",
    "    clage_noms = [\"tous_scol\", \"02_scol\", \"05_scol\", \"10_scol\", \"14_scol\", \"17_scol\", \"18_scol\"]\n",
    "    clage_noms_disp = [\"Tous\", \"0 - 2 ans\", \"3 - 5 ans\", \"6 - 10 ans\", \"11 - 14 ans\", \"15 - 17 ans\", \"plus 18 ans\"]\n",
    "    \n",
    "    for (idx, clage) in enumerate(clage_tranches):\n",
    "        clage_nom = clage_noms[idx]\n",
    "        data_incid_niveaux_scolaires_clage = data_incid_niveaux_scolaires[data_incid_niveaux_scolaires.age_18ans == clage].reset_index()\n",
    "\n",
    "        dict_data[clage_nom] = {}\n",
    "\n",
    "        taux_incidence = data_incid_niveaux_scolaires_clage[\"Ti\"]\n",
    "        taux_depistage = data_incid_niveaux_scolaires_clage[\"Td\"]\n",
    "        taux_positivite = data_incid_niveaux_scolaires_clage[\"Tp\"]\n",
    "        \n",
    "        dict_data[clage_nom][\"incidence\"] = {\"jour_nom\": \"jour_niveaux_scolaires\", \"valeur\": round(taux_incidence.astype(float)).astype(int).tolist()}\n",
    "        dict_data[clage_nom][\"depistage\"] = {\"jour_nom\": \"jour_niveaux_scolaires\", \"valeur\": round(taux_depistage.astype(float)).astype(int).tolist()}\n",
    "        dict_data[clage_nom][\"positivite\"] = {\"jour_nom\": \"jour_niveaux_scolaires\", \"valeur\": round(taux_positivite.astype(float), 1).tolist()}\n",
    "            \n",
    "    if export_jour:\n",
    "            dict_data[\"jour_niveaux_scolaires\"] = list(data_incid_niveaux_scolaires.jour.unique().astype(str))\n",
    "            dict_data[\"tranches_scolaires\"] = clage_tranches\n",
    "            dict_data[\"tranches_noms_scolaires\"] = clage_noms\n",
    "            dict_data[\"tranches_noms_affichage_scolaires\"] = clage_noms_disp\n",
    "\n",
    "    return dict_data\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_education(df_education):\n",
    "    clage_noms = [2, 5, 10, 14, 17, 18]\n",
    "    clage_disp = [\"0 - 2 ans\", \"3 - 5 ans\", \"6 - 10 ans\", \"11 - 14 ans\", \"15 - 17 ans\", \"Plus 18 ans\"]\n",
    "    \n",
    "    dict_data = {}\n",
    "    dict_data[\"tranches_noms\"] = clage_noms\n",
    "    dict_data[\"tranches_noms_affichage\"] = clage_disp\n",
    "    \n",
    "    for clage in clage_noms:\n",
    "        dict_data[clage] = {}\n",
    "        \n",
    "        df_temp = df_education[df_education[\"age_18ans\"]==clage]\n",
    "        \n",
    "        dict_data[clage][\"Ti\"] = df_temp[\"Ti\"].fillna(0).tolist()\n",
    "        dict_data[clage][\"Tp\"] = df_temp[\"Tp\"].fillna(0).tolist()\n",
    "        dict_data[clage][\"Td\"] = df_temp[\"Td\"].fillna(0).tolist()\n",
    "        \n",
    "    return dict_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_data(data, suffix=\"\"):\n",
    "    with open(PATH_STATS + 'dataexplorer{}.json'.format(suffix), 'w') as outfile:\n",
    "        json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataexplorer():\n",
    "    dict_data = {}\n",
    "    \n",
    "    regions = sorted(list(df_regions.regionName.unique()))\n",
    "    departements = list(df.dep.unique())\n",
    "    \n",
    "    dict_data[\"regions\"] = regions\n",
    "    dict_data[\"metropoles\"] = sorted(metropoles)\n",
    "    dict_data[\"departements\"] = departements\n",
    "    dict_data[\"france\"] = generate_data(df_incid_fra, df_france, df_sursaud_france, df_new_france, df_vue_ensemble, data_metropole=df_metro_0, data_vacsi=df_vacsi, data_obepine=pd.DataFrame(), data_opendata_indicateurs=df_opendata_indicateurs, mode=\"france\", export_jour=True, taux_croissance=True)\n",
    "    dict_data[\"metropole\"] = generate_data(df_incid[df_incid[\"dep\"].str.len()<=2].groupby([\"jour\"]).sum(), df[df[\"dep\"].str.len()<=2].groupby([\"jour\"]).sum(), df_sursaud[df_sursaud[\"dep\"].str.len()<=2].groupby([\"date_de_passage\"]).sum(), df_new[df_new[\"dep\"].str.len()<=2].groupby([\"jour\"]).sum(), data_vacsi=df_vacsi_dep[df_new[\"dep\"].str.len()<=2].groupby([\"jour\"]).sum())\n",
    "    dict_data[\"drom_com\"] = generate_data(df_incid[df_incid[\"dep\"].str.len()>2].groupby([\"jour\"]).sum(), df[df[\"dep\"].str.len()>2].groupby([\"jour\"]).sum(), df_sursaud[df_sursaud[\"dep\"].str.len()>2].groupby([\"date_de_passage\"]).sum(), df_new[df_new[\"dep\"].str.len()>2].groupby([\"jour\"]).sum(), data_vacsi=df_vacsi_dep[df_new[\"dep\"].str.len()>2].groupby([\"jour\"]).sum())\n",
    "\n",
    "    noms_departements={}\n",
    "    \n",
    "    for reg in regions:\n",
    "        print(reg)\n",
    "        data_hosp = df_regions[df_regions.regionName==reg]\n",
    "        dict_data[reg] = generate_data(data_incid=df_incid_regions[df_incid_regions.regionName==reg], \\\n",
    "                                       data_hosp=data_hosp,\\\n",
    "                                       data_sursaud=df_sursaud_regions[df_sursaud_regions.regionName==reg],\n",
    "                                       data_new=df_new_regions[df_new_regions.regionName==reg],\n",
    "                                       data_vacsi=df_vacsi_reg[df_vacsi_reg.regionName==reg],\\\n",
    "                                       data_obepine=pd.DataFrame() #df_obepine[df_obepine.regionName==reg]\n",
    "                                      )\n",
    "    print(\"Regions : done\")\n",
    "    \n",
    "    for dep in departements:\n",
    "        print(dep)\n",
    "        df_incid_dep = df_incid[df_incid.dep==dep]\n",
    "        df_dep = df[df.dep==dep]\n",
    "        dict_data[dep] = generate_data(data_incid=df_incid_dep, \n",
    "                                       data_hosp=df_dep,\n",
    "                                       data_sursaud=df_sursaud[df_sursaud.dep==dep],\n",
    "                                       data_new=df_new[df_new.dep==dep],\n",
    "                                       data_vacsi=df_vacsi_dep[df_vacsi_dep.dep==dep])\n",
    "        noms_departements[dep] = df_dep[\"departmentName\"].values[0]\n",
    "    dict_data[\"departements_noms\"] = noms_departements\n",
    "    \n",
    "    for zone in [zone_a, zone_b, zone_c]:\n",
    "        df_incid_zone = df_incid[df_incid.dep.isin(zone)].groupby(\"jour\").sum().reset_index()\n",
    "        df_zone = df[df.dep.isin(zone)].groupby(\"jour\").sum().reset_index()\n",
    "        df_sursaud_zone = df_sursaud[df_sursaud.dep.isin(zone)].groupby(\"date_de_passage\").sum().reset_index()\n",
    "        df_new_zone = df_new[df_new.dep.isin(zone)].groupby(\"jour\").sum().reset_index()\n",
    "        df_vacsi_zone = df_vacsi_dep[df_vacsi_dep.dep.isin(zone)].groupby(\"jour\").sum().reset_index()\n",
    "        \n",
    "        dict_data[zone[0]] = generate_data(df_incid_zone, df_zone, df_sursaud_zone, df_new_zone, data_vacsi=df_vacsi_zone)\n",
    "    \n",
    "    # Confinés mars 2021\n",
    "    df_incid_zone = df_incid[df_incid.dep.isin(confines_mars_2021)].groupby(\"jour\").sum().reset_index()\n",
    "    df_zone = df[df.dep.isin(confines_mars_2021)].groupby(\"jour\").sum().reset_index()\n",
    "    df_sursaud_zone = df_sursaud[df_sursaud.dep.isin(confines_mars_2021)].groupby(\"date_de_passage\").sum().reset_index()\n",
    "    df_new_zone = df_new[df_new.dep.isin(confines_mars_2021)].groupby(\"jour\").sum().reset_index()\n",
    "    df_vacsi_zone = df_vacsi_dep[df_vacsi_dep.dep.isin(confines_mars_2021)].groupby(\"jour\").sum().reset_index()\n",
    "    \n",
    "    dict_data[\"confines_mars_2021\"] = generate_data(df_incid_zone, df_zone, df_sursaud_zone, df_new_zone, data_vacsi=df_vacsi_zone)\n",
    "        \n",
    "    for metropole in metropoles:\n",
    "        print(metropole)\n",
    "        dict_data[metropole] = generate_data(data_metropole=df_metro_0[df_metro_0.Metropole == metropole], mode=\"metropoles\")\n",
    "        \n",
    "    dict_data[\"zones_vacances\"] = [\"zone_a\", \"zone_b\", \"zone_c\"]\n",
    "    \n",
    "    export_data(dict_data, suffix=\"_compr\")\n",
    "    export_data(dict_data[\"france\"], suffix=\"_compr_france\")\n",
    "    return dict_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataexplorer_age():\n",
    "    dict_data = {}\n",
    "    regions_tests_viros = list(dict.fromkeys(list(df_tests_viros_enrichi['regionName'].dropna().values))) \n",
    "    departements_tests_viros = list(dict.fromkeys(list(df_tests_viros_enrichi['dep'].dropna().values))) \n",
    "    dict_data[\"regions\"] = sorted(regions_tests_viros)\n",
    "    dict_data[\"departements\"] = sorted(departements_tests_viros)\n",
    "    \n",
    "    dict_data[\"france\"] = generate_data_age(df_tests_viros_france, df_hosp_clage_france, data_adm_hosp_clage=df_adm_hosp_clage_france, export_jour=True)\n",
    "    \n",
    "    for reg in regions_tests_viros:\n",
    "        dict_data[reg] = generate_data_age(df_tests_viros_regions[df_tests_viros_regions.regionName == reg],\\\n",
    "                                           df_hosp_clage_regions[df_hosp_clage_regions.regionName == reg]) #data_adm_hosp_clage=data_adm_hosp_clage\n",
    "    noms_departements={}\n",
    "    for dep in departements_tests_viros:\n",
    "        df_tests_viros_enrichi_temp = df_tests_viros_enrichi[df_tests_viros_enrichi.dep == dep]\n",
    "        dict_data[dep] = generate_data_age(df_tests_viros_enrichi_temp,\\\n",
    "                                           pd.DataFrame())\n",
    "        \n",
    "        nom_dep = df_tests_viros_enrichi_temp[\"departmentName\"].values[0]\n",
    "        \n",
    "        if(type(nom_dep) is float): #Pas de nom, nom_dep == NaN\n",
    "            #print(dep)\n",
    "            nom_dep = \"--\"\n",
    "        \n",
    "        noms_departements[dep] = nom_dep\n",
    "        \n",
    "    dict_data[\"departements_noms\"] = noms_departements\n",
    "    \n",
    "    export_data(dict_data, suffix=\"_compr_age\")\n",
    "    return dict_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataexplorer_education():\n",
    "    dict_data = {}\n",
    "    dict_data[\"france\"] = generate_data_education(df_education=df_education)\n",
    "    export_data(dict_data, suffix=\"_education\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataexplorer_niveaux_scolaires():\n",
    "    df_niveaux_scolaires_dep_sorted = df_niveaux_scolaires_dep.sort_values(by=\"dep\")\n",
    "    df_niveaux_scolaires_reg_sorted = df_niveaux_scolaires_reg.sort_values(by=\"reg\")\n",
    "    \n",
    "    dict_data = {}\n",
    "    \n",
    "    dict_data[\"departements_noms\"] = list(df_niveaux_scolaires_dep_sorted[\"departmentName\"].unique().astype(str))\n",
    "    dict_data[\"departements\"] = list(df_niveaux_scolaires_dep_sorted[\"dep\"].unique().astype(str))\n",
    "    \n",
    "    dict_data[\"regions_noms\"] = list(df_niveaux_scolaires_reg_sorted[\"regionName\"].unique().astype(str))\n",
    "    dict_data[\"regions\"] = dict_data[\"regions_noms\"] #list(df_niveaux_scolaires_reg_sorted[\"reg\"].unique().astype(str))\n",
    "    \n",
    "    dict_data[\"france\"] = generate_data_niveaux_scolaires(data_incid_niveaux_scolaires=df_niveaux_scolaires_fra, export_jour=True)\n",
    "    \n",
    "    for dep in dict_data[\"departements\"]:\n",
    "        data_incid_niveaux_scolaires_dep_temp = df_niveaux_scolaires_dep[df_niveaux_scolaires_dep[\"dep\"] == dep]\n",
    "        dict_data[dep] = generate_data_niveaux_scolaires(data_incid_niveaux_scolaires=data_incid_niveaux_scolaires_dep_temp, export_jour=False)\n",
    "        \n",
    "    for reg in dict_data[\"regions_noms\"]:\n",
    "        print(reg)\n",
    "        data_incid_niveaux_scolaires_reg_temp = df_niveaux_scolaires_reg[df_niveaux_scolaires_reg[\"regionName\"] == reg]\n",
    "        dict_data[reg] = generate_data_niveaux_scolaires(data_incid_niveaux_scolaires=data_incid_niveaux_scolaires_reg_temp, export_jour=False)\n",
    "    \n",
    "    export_data(dict_data, suffix=\"_education\")\n",
    "    return dict_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_nouveau_dashboard_france():\n",
    "    data={}\n",
    "    name=\"hospitalisations_par_age\"\n",
    "    \n",
    "    df = df_hosp_clage_france[df_hosp_clage_france[\"jour\"]==df_hosp_clage_france[\"jour\"].max()]\n",
    "    df = df[df[\"cl_age90\"] != 0].sort_values(by=\"cl_age90\")\n",
    "    \n",
    "    data[\"cl_age90\"] = [\"0-9 ans\", \"10-19 ans\", \"20-29 ans\", \"30-39 ans\", \"40-49 ans\", \"50-59 ans\", \"60-69 ans\", \"70-79 ans\", \"80-89 ans\", \"+ 90 ans\"]\n",
    "    data[\"hosp\"] = df[\"hosp\"].fillna(0).tolist()\n",
    "    data[\"rea\"] = df[\"rea\"].fillna(0).tolist()\n",
    "    \n",
    "    with open(PATH_STATS + 'api/{}.json'.format(name), 'w') as outfile:\n",
    "        json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nouveau_dashboard_france()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ns = dataexplorer_niveaux_scolaires()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dict_dataexplorer = dataexplorer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_data = dataexplorer_age()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objectif_deconfinement():\n",
    "    dict_json = {}\n",
    "    n = 70\n",
    "    \n",
    "    ## HOSP\n",
    "    struct = {\"dates\": [], \"values\": []}\n",
    "    dict_json[\"hosp\"] = struct\n",
    "    dict_json[\"hosp\"][\"values\"] = [float(round(x, 1)) for x in df_france[\"hosp\"].values[-n:]]\n",
    "    dict_json[\"hosp\"][\"dates\"] = list(df_france[\"jour\"].values[-n:])\n",
    "    \n",
    "    ## HOSP ADM\n",
    "    struct = {\"dates\": [], \"values\": []}\n",
    "    dict_json[\"adm_hosp\"] = struct\n",
    "    dict_json[\"adm_hosp\"][\"values\"] = [float(round(x, 1))for x in df_new_france[\"incid_hosp\"].rolling(window=7).mean().dropna(0).values[-n:]]\n",
    "    dict_json[\"adm_hosp\"][\"dates\"] = list(df_new_france[\"jour\"].values[-n:])\n",
    "    \n",
    "    ## REA\n",
    "    struct = {\"dates\": [], \"values\": []}\n",
    "    dict_json[\"rea\"] = struct\n",
    "    dict_json[\"rea\"][\"values\"] = [float(round(x, 1)) for x in df_france[\"rea\"].values[-n:]]\n",
    "    dict_json[\"rea\"][\"dates\"] = list(df_france[\"jour\"].values[-n:])\n",
    "    \n",
    "    ## REA ADM\n",
    "    struct = {\"dates\": [], \"values\": []}\n",
    "    dict_json[\"adm_rea\"] = struct\n",
    "    dict_json[\"adm_rea\"][\"values\"] = [float(round(x, 1)) for x in df_new_france[\"incid_rea\"].rolling(window=7).mean().dropna(0).values[-n:]]\n",
    "    dict_json[\"adm_rea\"][\"dates\"] = list(df_new_france[\"jour\"].values[-n:])\n",
    "    \n",
    "    ## DC\n",
    "    struct = {\"dates\": [], \"values\": []}\n",
    "    dict_json[\"dc\"] = struct\n",
    "    dict_json[\"dc\"][\"values\"] = [float(round(x, 1)) for x in df_new_france[\"incid_dc\"].rolling(window=7).mean().fillna(0).values[-n:]]\n",
    "    dict_json[\"dc\"][\"dates\"] = list(df_france[\"jour\"].values[-n:])\n",
    "    \n",
    "    ## Cas\n",
    "    struct = {\"date\": \"\", \"values\": []}\n",
    "    dict_json[\"cas\"] = struct\n",
    "    cas_rolling = df_incid_fra[\"P\"].rolling(window=7, center=False).mean().dropna()\n",
    "    \n",
    "    dict_json[\"cas\"][\"values\"] = [float(round(x, 1)) for x in cas_rolling.values[-n:]]\n",
    "    dict_json[\"cas\"][\"dates\"] = list(df_incid_fra.loc[cas_rolling.index.values[-n:], \"jour\"])\n",
    "    \n",
    "    ## Cas date publication\n",
    "    struct = {\"date\": \"\", \"values\": []}\n",
    "    dict_json[\"cas_spf\"] = struct\n",
    "    #cas_rolling = df_vue_ensemble[\"total_cas_confirmes\"].diff().rolling(window=7, center=False).mean().fillna(0)\n",
    "    cas_rolling = df_opendata_indicateurs[\"conf_j1\"].rolling(window=7, center=False).mean().fillna(0)\n",
    "\n",
    "    dict_json[\"cas_spf\"][\"values\"] = [float(round(x, 1)) for x in cas_rolling.values[-n:]]\n",
    "    dict_json[\"cas_spf\"][\"dates\"] = list(df_opendata_indicateurs.loc[cas_rolling.index.values[-n:], \"date\"])\n",
    "    \n",
    "    with open(PATH_STATS + 'objectif_deconfinement.json', 'w') as outfile:\n",
    "        json.dump(dict_json, outfile)\n",
    "        \n",
    "objectif_deconfinement()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
